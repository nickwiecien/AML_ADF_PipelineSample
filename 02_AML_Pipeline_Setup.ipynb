{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "111a4fb4-677c-4e21-828b-1c238521ef7b",
   "metadata": {},
   "source": [
    "# 02. Azure ML Pipeline Creation\n",
    "This notebook demonstrates creation of an Azure ML pipeline which is designed to receive two datasets as inputs (both tabular data), apply some transformations, and export the end results to a text file in an Azure ML-linked datastore. The custom python code within this single python step can be adapted to support numerous operations including model scoring."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60584a2-d500-44d1-8d58-ba0351e2a7c5",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fbf0c4-f9b8-406d-94ba-903d123043ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset, Model\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute, DatabricksCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData, PipelineDataset\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "from azureml.pipeline.steps import DatabricksStep\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0c095-b842-4279-ba3b-a407c39ebbc4",
   "metadata": {},
   "source": [
    "### Connect to AML workspace and get references to training cluster and default datastore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8029d96-a668-4c53-92b5-afb86cde9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "compute_target  = ComputeTarget(workspace=ws, name='cpucluster')\n",
    "\n",
    "#Get default datastore\n",
    "default_ds = ws.get_default_datastore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddd8eb7-7d87-48c5-ab20-a310ea904ce9",
   "metadata": {},
   "source": [
    "### Create RunConfiguration for pipeline execution\n",
    "The `RunConfiguration` is essentially the environment in which a pipeline script will execute. Here we are using a prepared Azure ML environment though custom environments can be configured"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77476f9-f84a-4b67-99f5-ab8b73494971",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfiguration()\n",
    "run_config.docker.use_docker = True\n",
    "run_config.environment = Environment.get(ws, name='AzureML-sklearn-1.0-ubuntu20.04-py38-cpu', version=\"1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33347152-d2a3-496e-8e74-094d55447477",
   "metadata": {},
   "source": [
    "### Define Input and Output Datasets\n",
    "Retrieve references to two registered datasets to be consumed as inputs by the pipeline, and configure an `OutputFileDatasetConfig` to point to a location in blob storage where the pipeline output will be written. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d8ab6e-2946-44d5-bf8e-166a7bc64731",
   "metadata": {},
   "outputs": [],
   "source": [
    "scored_data = OutputFileDatasetConfig(name='Scored_Data', destination=(default_ds, 'scored_data'))\n",
    "dataset_one = Dataset.get_by_name(ws, 'Dataset_One')\n",
    "dataset_two = Dataset.get_by_name(ws, 'Dataset_Two')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd19ce65-a7f3-4abe-adeb-e3a88142a927",
   "metadata": {},
   "source": [
    "### Define Pipeline Parameters\n",
    "Pipeline parameters are dynamic arguments which can be passed to a pipeline at runtime. Here we are passing a single argument, `filename` which represents the name of the file that should be written to blob storage. To ensure files are not overwritten these files are named with a 'timestamp.csv' notation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9302595b-61f5-4182-a74b-c3b1ac85546e",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = PipelineParameter(name='filename', default_value='20220506000001.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4405c38b-f901-4eac-aae2-65160a0c59d6",
   "metadata": {},
   "source": [
    "### Define Pipeline Steps\n",
    "Pipeline steps are module steps designed to be executed in sequence to perform a machine learning operation. Here we have a single `PythonScriptStep` which simply executes a python script that contains logic for accepting two input datasets, performing some transformations, and exporting the results into a CSV file. This python file is located at `./pipeline_step_scripts/score_data.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0067055f-8cee-461f-a66d-e5c466ece41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_data_step = PythonScriptStep(\n",
    "    name='Score Data',\n",
    "    script_name='score_data.py',\n",
    "    arguments=['--scored_data', scored_data, '--filename', filename],\n",
    "    inputs=[dataset_one.as_named_input('Dataset_One'), dataset_two.as_named_input('Dataset_Two')],\n",
    "    outputs=[scored_data],\n",
    "    compute_target=compute_target,\n",
    "    source_directory='./pipeline_step_scripts',\n",
    "    allow_reuse=False,\n",
    "    runconfig=run_config\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7a4dbf-8ff2-4348-b79b-1185c9d5c986",
   "metadata": {},
   "source": [
    "### Define Pipeline\n",
    "Azure ML Pipelines are series of steps designed to be executed in sequence. The syntax for constructing individual steps into a unified pipeline can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78630dc-a58f-4f25-8880-06a1ee74f212",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(workspace=ws, steps=[score_data_step])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd92ad24-c0e9-4fb8-a8ed-f50ddb25661c",
   "metadata": {},
   "source": [
    "### Create Published PipelineEndpoint\n",
    "`PipelineEndpoints` can be used to create a versions of published pipelines while maintaining a consistent endpoint. These endpoint URLs can be triggered remotely by submitting an authenticated request and updates to the underlying pipeline are tracked in the AML workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a85c8e-773a-4b86-9978-d1ffb3608215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.pipeline.core import PipelineEndpoint\n",
    "\n",
    "def published_pipeline_to_pipeline_endpoint(\n",
    "    workspace,\n",
    "    published_pipeline,\n",
    "    pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=\"Endpoint to my pipeline\",\n",
    "):\n",
    "    try:\n",
    "        pipeline_endpoint = PipelineEndpoint.get(\n",
    "            workspace=workspace, name=pipeline_endpoint_name\n",
    "        )\n",
    "        print(\"using existing PipelineEndpoint...\")\n",
    "        pipeline_endpoint.add_default(published_pipeline)\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "        # create PipelineEndpoint if it doesn't exist\n",
    "        print(\"PipelineEndpoint does not exist, creating one for you...\")\n",
    "        pipeline_endpoint = PipelineEndpoint.publish(\n",
    "            workspace=workspace,\n",
    "            name=pipeline_endpoint_name,\n",
    "            pipeline=published_pipeline,\n",
    "            description=pipeline_endpoint_description\n",
    "        )\n",
    "\n",
    "\n",
    "pipeline_endpoint_name = 'Sample Scoring Pipeline'\n",
    "pipeline_endpoint_description = ''\n",
    "\n",
    "published_pipeline = pipeline.publish(name=pipeline_endpoint_name,\n",
    "                                     description=pipeline_endpoint_description,\n",
    "                                     continue_on_step_failure=False)\n",
    "\n",
    "published_pipeline_to_pipeline_endpoint(\n",
    "    workspace=ws,\n",
    "    published_pipeline=published_pipeline,\n",
    "    pipeline_endpoint_name=pipeline_endpoint_name,\n",
    "    pipeline_endpoint_description=pipeline_endpoint_description\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
