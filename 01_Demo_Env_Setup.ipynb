{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0567ab-8813-4c2d-9288-d7a7e3029a50",
   "metadata": {},
   "source": [
    "# 01. Azure ML/Azure Data Factory Demo - Environment Setup\n",
    "This notebook (designed to run from an Azure ML Compute Instance) connects to an AML workspace, and registers two tabular datasets from CSVs located within the `./sample_data` directory. These datasets (the Boston home prices and Iris Setosa datasets, respectively) are intended purely to be representative datasets that can be retrieved from the workspace and consumed within an AML pipeline. The intention of this demo is to create and execute an AML pipeline which accepts a single argument, and writes a file to an AML-linked blob store. This file can then be copied into an Azure SQL database for consumption in downstream reports. The final cells in this notebook create a table in an Azure SQL database (using pyodbc) that can serve as an effective sink for these data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4481233a-be18-4307-b67f-dea8e0642a3d",
   "metadata": {},
   "source": [
    "### Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e018e04d-03eb-40eb-a4a7-c55f2f1dbc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Experiment, Datastore, Environment, Dataset\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute, DataFactoryCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DEFAULT_CPU_IMAGE\n",
    "from azureml.pipeline.core import Pipeline, PipelineParameter, PipelineData\n",
    "from azureml.pipeline.steps import PythonScriptStep\n",
    "from azureml.pipeline.core import PipelineParameter, PipelineData\n",
    "from azureml.data.output_dataset_config import OutputTabularDatasetConfig, OutputDatasetConfig, OutputFileDatasetConfig\n",
    "from azureml.data.datapath import DataPath\n",
    "from azureml.data.data_reference import DataReference\n",
    "from azureml.data.sql_data_reference import SqlDataReference\n",
    "from azureml.pipeline.steps import DataTransferStep\n",
    "import logging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9473a97-8140-457f-a46a-f14de71cd672",
   "metadata": {},
   "source": [
    "### Connect to AML workspace, provision compute cluster for pipeline execution, and get references to default datastore and Key Vault"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c419d4-6071-4459-b53a-be5bdf33b783",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to AML Workspace\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "#Select AML Compute Cluster\n",
    "cpu_cluster_name = 'cpucluster'\n",
    "\n",
    "# Verify that cluster does not exist already\n",
    "try:\n",
    "    cpu_cluster = ComputeTarget(workspace=ws, name=cpu_cluster_name)\n",
    "    print('Found an existing cluster, using it instead.')\n",
    "except ComputeTargetException:\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D13_V2',\n",
    "                                                           min_nodes=0,\n",
    "                                                           max_nodes=1)\n",
    "    cpu_cluster = ComputeTarget.create(ws, cpu_cluster_name, compute_config)\n",
    "    cpu_cluster.wait_for_completion(show_output=True)\n",
    "    \n",
    "#Create Data Factory Compute for DataTransferStep\n",
    "def get_or_create_data_factory(workspace, factory_name):\n",
    "    try:\n",
    "        return DataFactoryCompute(workspace, factory_name)\n",
    "    except ComputeTargetException as e:\n",
    "        if 'ComputeTargetNotFound' in e.message:\n",
    "            print('Data factory not found, creating...')\n",
    "            provisioning_config = DataFactoryCompute.provisioning_configuration()\n",
    "            data_factory = ComputeTarget.create(workspace, factory_name, provisioning_config)\n",
    "            data_factory.wait_for_completion()\n",
    "            return data_factory\n",
    "        else:\n",
    "            raise e\n",
    "data_factory_name = 'adfcompute'           \n",
    "data_factory_compute = get_or_create_data_factory(ws, data_factory_name)\n",
    "\n",
    "#Get Default Datastore\n",
    "default_ds = ws.get_default_datastore()\n",
    "\n",
    "# Get Default Key Vault\n",
    "default_kv = ws.get_default_keyvault()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1f54c3-d0bb-4633-bf58-36ac8ad0d6aa",
   "metadata": {},
   "source": [
    "### Create Datasets\n",
    "Upload CSVs from the `./sample_data` directory (Boston Home Prices and Iris Setosa, respectively) and register as tabular datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0f44d8-a123-4534-8a62-c60e861807e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two datasets in default storage account\n",
    "default_ds.upload('./sample_data', target_path='sample_data', overwrite=True)\n",
    "\n",
    "dataset_1 = Dataset.Tabular.from_delimited_files((default_ds, 'sample_data/a.csv'))\n",
    "dataset_1.register(ws, 'Dataset_One', create_new_version=True)\n",
    "\n",
    "dataset_2 = Dataset.Tabular.from_delimited_files((default_ds, 'sample_data/b.csv'))\n",
    "dataset_2.register(ws, 'Dataset_Two', create_new_version=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64024d-4fee-448d-b769-4e1480d5440f",
   "metadata": {},
   "source": [
    "### Set secrets in Azure Key Vault\n",
    "The sample code below is configured to add Azure SQL credential details (server name, database name, username and password) to an Azure Key Vault for secure storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e325d9e-826c-401b-87cb-91734115cb42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_kv.set_secret('SQL-SERVER', 'YOUR-SERVER-NAME')\n",
    "# default_kv.set_secret('SQL-DATABASE', 'YOUR-DATABASE-NAME')\n",
    "# default_kv.set_secret('SQL-USERNAME', 'YOUR-USERNAME')\n",
    "# default_kv.set_secret('SQL-PASSWORD', 'YOUR-PASSWORD')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75ea273-a60e-4958-ac54-1a4e2f63cc99",
   "metadata": {},
   "source": [
    "### Create Table in Azure SQL Database\n",
    "Create a table inside an Azure SQL database that results will eventually be written to via Azure Data Factory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790cab2d-6d04-43d2-b80e-b05174fe1e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc \n",
    "server = default_kv.get_secret('SQL-SERVER')\n",
    "database = default_kv.get_secret('SQL-DATABASE')\n",
    "username = default_kv.get_secret('SQL-USERNAME')\n",
    "password = default_kv.get_secret('SQL-PASSWORD') \n",
    "with pyodbc.connect('DRIVER={ODBC Driver 17 for SQL Server};SERVER='+server+';DATABASE='+database+';UID='+username+';PWD='+ password) as cnxn:\n",
    "    cursor = cnxn.cursor()\n",
    "\n",
    "    sql_command = \"\"\"\n",
    "    CREATE TABLE mydata (A float, B float, TIME datetime)\n",
    "    \"\"\"\n",
    "    cursor.execute(sql_command)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 - AzureML",
   "language": "python",
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
